{
  "hash": "e19adfdf635f92307cec489b7917bbc7",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"\"\n---\n\n\nThe DAPC aims to partition variation into components between and within populations, emphasizing maximization of between-population variation and minimization of within-population variation. As a result, DAPC can also probabilistically assign individuals to populations, similar to Bayesian clustering methods (Milgroom, 2015).\n\nThis analysis should be conducted using **R Studio software**.\n\nLoad the required libraries\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(adegenet)\nlibrary(grDevices)\nlibrary(seqinr)\n```\n:::\n\n\nLoad the data\n\n\n::: {.cell}\n\n```{.r .cell-code}\nobj <- fasta2genlight(\"alinhamentocerto.fas\")\n```\n:::\n\n\nTo define the population in my **obj** object, where I have 27 individuals belonging to group 1 and 32 belonging to group 2, I did:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nx <- pop(obj) <- c(\"grupo1\", \"grupo1\", \"grupo1\", \"grupo1\", \"grupo1\", \"grupo1\", \"grupo1\", \"grupo1\", \"grupo1\",\"grupo1\", \"grupo1\", \"grupo1\", \"grupo1\", \"grupo1\", \"grupo1\", \"grupo1\", \"grupo1\", \"grupo1\",\"grupo1\", \"grupo1\", \"grupo1\",  \"grupo1\", \"grupo1\",\"grupo1\", \"grupo1\", \"grupo1\", \"grupo1\", \"grupo2\", \"grupo2\", \"grupo2\",\"grupo2\", \"grupo2\", \"grupo2\",\"grupo2\", \"grupo2\", \"grupo2\",  \"grupo2\", \"grupo2\", \"grupo2\", \"grupo2\", \"grupo2\", \"grupo2\", \"grupo2\", \"grupo2\", \"grupo2\",\"grupo2\", \"grupo2\", \"grupo2\", \"grupo2\", \"grupo2\", \"grupo2\", \"grupo2\", \"grupo2\", \"grupo2\", \"grupo2\", \"grupo2\", \"grupo2\", \"grupo2\", \"grupo2\") \n```\n:::\n\n\nSpecifying the number of PCs to attempt n.pca. Based on the results, we can restrict the number of PCs to 10\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(999)\npramx <- xvalDapc(tab(obj, NA.method = \"mean\"), pop(obj))\n```\n\n::: {.cell-output-display}\n![](DAPC_files/figure-html/unnamed-chunk-4-1.png){width=672}\n:::\n:::\n\n\nHere I am already restricting it to 10 for n.pca.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(999)\nsystem.time(pramx <- xvalDapc(tab(obj, NA.method = \"mean\"), pop(obj),\n                             n.pca = 0:10, n.rep = 100,\n                             parallel = \"multicore\", ncpus = 4L))\n```\n\n::: {.cell-output-display}\n![](DAPC_files/figure-html/unnamed-chunk-5-1.png){width=672}\n:::\n\n::: {.cell-output .cell-output-stdout}\n\n```\n  usuÃ¡rio   sistema decorrido \n     4.84      0.52     11.51 \n```\n\n\n:::\n:::\n\n\nThe results show that the first PC explains a significant portion of my findings\n\n\n::: {.cell}\n\n```{.r .cell-code}\npramx[-1]\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n$`Median and Confidence Interval for Random Chance`\n     2.5%       50%     97.5% \n0.3854167 0.5049190 0.6244213 \n\n$`Mean Successful Assignment by Number of PCs of PCA`\n        1         2         3         4         5         6         7         8 \n0.4983333 0.4350000 0.4000000 0.3633333 0.3416667 0.3783333 0.4250000 0.4650000 \n        9        10        11        12        13        14        15 \n0.4600000 0.4816667 0.4950000 0.4533333 0.4300000 0.4433333 0.4550000 \n\n$`Number of PCs Achieving Highest Mean Success`\n[1] \"1\"\n\n$`Root Mean Squared Error by Number of PCs of PCA`\n        1         2         3         4         5         6         7         8 \n0.5019407 0.5780715 0.6146363 0.6527719 0.6739189 0.6409628 0.5904330 0.5639641 \n        9        10        11        12        13        14        15 \n0.5661763 0.5474689 0.5288877 0.5739725 0.5976807 0.5797509 0.5678908 \n\n$`Number of PCs Achieving Lowest MSE`\n[1] \"1\"\n\n$DAPC\n\t#################################################\n\t# Discriminant Analysis of Principal Components #\n\t#################################################\nclass: dapc\n$call: dapc.data.frame(x = as.data.frame(x), grp = ..1, n.pca = ..2, \n    n.da = ..3)\n\n$n.pca: 1 first PCs of PCA used\n$n.da: 1 discriminant functions saved\n$var (proportion of conserved variance): 0.489\n\n$eig (eigenvalues): 0.001275  vector    length content                   \n1 $eig      1      eigenvalues               \n2 $grp      59     prior group assignment    \n3 $prior    2      prior group probabilities \n4 $assign   59     posterior group assignment\n5 $pca.cent 212    centring vector of PCA    \n6 $pca.norm 212    scaling vector of PCA     \n7 $pca.eig  25     eigenvalues of PCA        \n\n  data.frame    nrow ncol content                                          \n1 $tab          59   1    retained PCs of PCA                              \n2 $means        2    1    group means                                      \n3 $loadings     1    1    loadings of variables                            \n4 $ind.coord    59   1    coordinates of individuals (principal components)\n5 $grp.coord    2    1    coordinates of groups                            \n6 $posterior    59   2    posterior membership probabilities               \n7 $pca.loadings 212  1    PCA loadings of original variables               \n8 $var.contr    212  1    contribution of original variables               \n```\n\n\n:::\n:::\n\n\nNow that I have defined my number of PCs, I need to determine the optimal number of clusters. I set `choose = TRUE`, and when prompted in the console to `'Choose the number of clusters (>=2)'`, I will enter 10. This will allow me to obtain BIC values for up to 10 groups. Based on the BIC values, I will analyze the best number of clusters. Starting from 7, lower BIC values indicate better model fit\n\n\n::: {.cell}\n\n```{.r .cell-code}\nt <- find.clusters(obj, max.n.clust=10, n.pca = 200, choose = TRUE)\n```\n\n::: {.cell-output-display}\n![](DAPC_files/figure-html/unnamed-chunk-7-1.png){width=672}\n:::\n\n::: {.cell-output .cell-output-stdout}\n\n```\nChoose the number of clusters (>=2): \n```\n\n\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nmaxK <- 10\nmyMat <- matrix(nrow=10, ncol=maxK)\ncolnames(myMat) <- 1:ncol(myMat)\nfor(i in 1:nrow(myMat)){\n  grp <- find.clusters(obj, n.pca = 40, choose.n.clust = FALSE,  max.n.clust = maxK)\n  myMat[i,] <- grp$Kstat\n}\n\nlibrary(ggplot2)\nlibrary(reshape2)\nmy_df <- melt(myMat)\ncolnames(my_df)[1:3] <- c(\"Group\", \"K\", \"BIC\")\nmy_df$K <- as.factor(my_df$K)\nhead(my_df)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n  Group K      BIC\n1     1 1 164.9196\n2     2 1 164.9196\n3     3 1 164.9196\n4     4 1 164.9196\n5     5 1 164.9196\n6     6 1 164.9196\n```\n\n\n:::\n\n```{.r .cell-code}\np1 <- ggplot(my_df, aes(x = K, y = BIC))\np1 <- p1 + geom_boxplot()\np1 <- p1 + theme_bw()\np1 <- p1 + xlab(\"Number of groups (K)\")\np1\n```\n\n::: {.cell-output-display}\n![](DAPC_files/figure-html/unnamed-chunk-8-1.png){width=672}\n:::\n:::\n\n\nI started assuming it's 7\n\n\n::: {.cell}\n\n```{.r .cell-code}\nt <- find.clusters(obj, max.n.clust=10, n.pca = 200, choose = TRUE, n.clust = 7)\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nhead(t$grp, 7)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n UFV34 UFV486 UFV443 UFV445 UFV446  UFV72 UFV519 \n     3      7      6      4      4      6      4 \nLevels: 1 2 3 4 5 6 7\n```\n\n\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nt$size\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1]  1  6  3  7  3 24 15\n```\n\n\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\ndapc1 <- dapc(obj, t$grp, n.pca = 2, n.da = 2, var.contr = TRUE, scale = FALSE)\n```\n:::\n\n\nHere we can observe overlap between groups 2 and 6, indicating they belong to the same cluster. Therefore, I will test for n.clust=6\n\n\n::: {.cell}\n\n```{.r .cell-code}\nscatter(dapc1, ratio.pca=0.3, bg=\"white\", legend = TRUE, pch=20, cell=10, cstar=0, solid=.4, cex=3, clab=0,clabel = FALSE, posi.leg = \"bottomleft\", scree.pca = TRUE,\n        posi.pca = \"topleft\", cleg = 1, xax = 1, yax = 2, inset.solid = 0.4\n        )\n```\n\n::: {.cell-output-display}\n![](DAPC_files/figure-html/unnamed-chunk-13-1.png){width=672}\n:::\n:::\n\n\nTesting n.clust = 6 still classifies two groups as different when they are actually the same. Look at groups 6 and 5. I will try 5 clusters\n\n\n::: {.cell}\n\n```{.r .cell-code}\nt2 <- find.clusters(obj, max.n.clust=10, n.pca = 200, choose = TRUE, n.clust = 6)\ndapc2 <- dapc(obj, t2$grp, n.pca = 2, n.da = 2, var.contr = TRUE, scale = FALSE)\n\n\nscatter(dapc2, ratio.pca=0.3, bg=\"white\", legend = TRUE, pch=20, cell=8, cstar=0, solid=.4, cex=3, clab=0,clabel = FALSE, posi.leg = \"bottomleft\", scree.pca = TRUE,\n        posi.pca = \"topleft\", cleg = 1, xax = 1, yax = 2, inset.solid = 0.4\n        )\n```\n\n::: {.cell-output-display}\n![](DAPC_files/figure-html/unnamed-chunk-14-1.png){width=672}\n:::\n:::\n\n\nTesting n.clust = 5, there is no overlap between groups. It seems that this subdivision is appropriate but I will test with n=4\n\n\n::: {.cell}\n\n```{.r .cell-code}\nt3 <- find.clusters(obj, max.n.clust=10, n.pca = 200, choose = TRUE, n.clust =5)\ndapc3 <- dapc(obj, t3$grp, n.pca = 2, n.da = 2, var.contr = TRUE, scale = FALSE)\n\nscatter(dapc3, ratio.pca=0.3, bg=\"white\", legend = TRUE, pch=20, cell=10, cstar=0, solid=.4, cex=3, clab=0,clabel = FALSE, posi.leg = \"bottomleft\", scree.pca = TRUE,\n        posi.pca = \"topleft\", cleg = 1, xax = 1, yax = 2, inset.solid = 0.4\n        )\n```\n\n::: {.cell-output-display}\n![](DAPC_files/figure-html/unnamed-chunk-15-1.png){width=672}\n:::\n:::\n\n\nSo I decided to test n.clust = 4, and it was the smallest number of clusters that classified according to\n\n\n::: {.cell}\n\n```{.r .cell-code}\nt3 <- find.clusters(obj,  n.pca = 200,  n.clust =4)\ndapc3 <- dapc(obj, t3$grp, n.pca = 2, n.da = 2, var.contr = TRUE, scale = FALSE)\n\nscatter(dapc3, ratio.pca=0.3, bg=\"white\", legend = TRUE, pch=20, cell=7, cstar=0, solid=.4, cex=3, clab=0,clabel = FALSE, posi.leg = \"bottomleft\", scree.pca = TRUE,\n        posi.pca = \"topleft\", cleg = 1, xax = 1, yax = 2, inset.solid = 0.4)\n```\n\n::: {.cell-output-display}\n![](DAPC_files/figure-html/unnamed-chunk-16-1.png){width=672}\n:::\n:::\n\n\n**PCA Eigenvalues:** These are the eigenvalues associated with the principal components obtained through PCA analysis. They are important for determining how many principal components are needed to explain a significant amount of variance in the original data.\n\n**DA Eigenvalues:** These are the eigenvalues associated with the linear discriminants obtained through discriminant analysis. They are used to determine which discriminants capture the highest variance between classes.\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}